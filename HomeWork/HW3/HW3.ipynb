{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE 380L: Data Mining\n",
    "## Assignment 3\n",
    "\n",
    "Due: Feb 28 at 11:59 pm  \n",
    "Total points: 55\n",
    "\n",
    "You may work in pairs. Only one student per pair needs to submit the assignment on Canvas, but you should include names and UTEIDs for both students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liheng Ding(ld24529), Yu Sun(ys8797)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Ridge and Lasso Regression using R (15 points)\n",
    "To perform Ridge and Lasso regression in R, we will use the [glmnet](https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet_beta.html#lin) package.\n",
    "\n",
    "In this question, we use the diabetes dataset again. The dataset can be found at: http://www4.stat.ncsu.edu/~boos/var.select/diabetes.rwrite1.txt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## import the diabetes dataset\n",
    "diabetes <- read.table(\"/path/to/diabetes.txt\", sep=\" \", col.names=c(\"age\", \"sex\", \"bmi\", \"map\", \"tc\", \"ldl\", \"hdl\", \"tch\", \"ltg\", \"glu\", \"y\"), header=TRUE)\n",
    "X <- diabetes[, 1:10]\n",
    "y <- diabetes[, 11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the datasets, use the following code to add additional interaction variables to the features matrix. You should have 65 variables (including 55 interaction variables) and one target variable."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## add additional interaction variables to the features matrix\n",
    "new_X <- poly(as.matrix(X), degree=2, raw=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, you will explore the application of Lasso and Ridge regression using  package in R. The following code will split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## set the seed to make your partition reproductible\n",
    "set.seed(123)\n",
    "splitSample <- sample(1:2, size=nrow(new_X), prob=c(0.67,0.33))\n",
    "train_X <- new_X[splitSample==1,]\n",
    "test_X <- new_X[splitSample==2,]\n",
    "\n",
    "## Fit models:\n",
    "fit.ridge <- glmnet(x.train, y.train, family=\"gaussian\", alpha=0)\n",
    "fit.lasso <- glmnet(x.train, y.train, family=\"gaussian\", alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) (3 pts) Use the [\"pairs\"](http://stat.ethz.ch/R-manual/R-devel/library/graphics/html/pairs.html) function to see the scatter-plots of pairs of (the original set of 10) variables.\n",
    "\n",
    "(b) (5 pts) Use glmnet ridge and lasso regression models to do a 5-fold cross validation using [cvTools](https://cran.r-project.org/web/packages/cvTools/index.html) package. For the sweep of the regularization parameter, we will look at a grid of values ranging from $\\lambda = 10^{10}$ to $\\lambda = 10^{-2}$. In R, you can consider this range of values as follows:\n",
    "\n",
    "      alphas <- 10^seq(from = -2, to = 10, length.out = 100)\n",
    "\n",
    "  Report the best chosen $\\lambda$ based on cross validation. The cross validation should happen on your training data using  average MSE as the scoring metric.\n",
    "\n",
    "(c) (4 pts) Run ridge and lasso for all of the alphas specified above (on training data), and plot the coefficients learned for each of them - there should be one plot each for lasso and ridge, so a total of two plots; the plots for different features for a method should be on the same plot (e.g. Fig 6.6 of JW). What do you qualitatively observe when value of the regularization parameter is changed?\n",
    "\n",
    "(d) (3 pts) Run least squares regression, ridge, and lasso on the training data. For ridge and lasso, use only the best regularization parameter. Report the prediction error (MSE) on the test data for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Multi-level Model in Python (18 points)\n",
    "In this problem, you will explore multi-level model using a python package [PyMC3](https://pymc-devs.github.io/pymc3/index.html#). The dataset \"oxboys.csv\" will be used in this question.  This dataset contains three fields:\n",
    "- Individual ID\n",
    "- Age in years\n",
    "- Height in cm\n",
    "\n",
    "(a)  (2 pts) Plot the relationship between height and year, and draw a linearly regressed line ignoring the ID variable.\n",
    "\n",
    "(b)  (2 pts) Plot the relationship between height and year, but this time, fit a different linear regression for each individual.\n",
    "\n",
    "(c)  (2 + 4 + 6 pts) Divide the dataset into training and test sets.  The training set contains the first 7 years of the measurements, and the test set contains the rest of the measurements. \n",
    "Build three different linear models:\n",
    "- Global model:  a linear model ignoring the id variable. Pool all data and estimate one common regression model to assess the influence of Age across all Individuals' heights.\n",
    "- Local model:  a different linear model for each individual i.e., 26 different linear regressions. We are interested in whether different individuals actually follow separate regression models.\n",
    "- Multilevel model:  Use the [PyMC3](http://pymc-devs.github.io/pymc3/notebooks/GLM-hierarchical.html#Partial-pooling:-Hierarchical-Regression-aka,-the-best-of-both-worlds) package to fit a multilevel model specified as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{height}_{it} &= \\beta_{0i} + \\beta_{1i} \\text{year}_{it} + \\epsilon_{it}\\\\\n",
    "\\beta_{0i} &= \\beta_{00}  + \\eta_{0i} \\\\\n",
    "\\beta_{1i} &= \\beta_{10} + \\eta_{1i} \\\\\n",
    "\\begin{bmatrix} \\eta_{0i} \\\\ \\eta_{1i} \\end{bmatrix} &\\sim \\text{Bivariate Normal}(\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} \\sigma_{1}^2 & 0\\\\ 0 & \\sigma_{2}^2 \\end{bmatrix})\\\\\n",
    "\\epsilon_{it} &\\sim \\text{Normal}(0, \\sigma^2)\n",
    "\\end{align*}\n",
    "\n",
    "Predict the heights for the next 2 years, and calculate the mean squared errors from the three models.\n",
    "\n",
    "(d)  (2 pts) Briefly state what do $\\beta_{00}$ and $\\beta_{10}$ mean in this multilevel model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Sparse Linear Regression (15 points)\n",
    "In this question, we will explore a couple of modelling techniques for sparse linear regression. For this question,\n",
    "please make sure you have $numpy/scipy$ and $sklearn$ installed. Each student is to expected to run 3 sparse regression models:\n",
    "- Lasso (from the sklearn package)\n",
    "- Automatic Relevance Determination (ARD), available in the sklearn package, or use the provided file $iterative\\_ard.py$.\n",
    "- SubmodRegression. See provided file $submodRegression.py$. The function run submodregression\n",
    "takes as input the training data, test data and a matrix $C$ which is to be set as identity except for the fMRI dataset for which it is provided as detailed below.\n",
    "\n",
    "(5 + 5 + 5 pts) You are only expected to run the above 3 models on one of the following 4 datasets. Take the numeral part of your\n",
    "UTEID, and divide it by 4. Use the remainder to select the corresponding dataset you need to work with:-\n",
    "\n",
    "0. Simulated data: filename is $simulated.npz$. Use $data = numpy.load(simulated.npz)$ to access the serialized file. Then access features using $data[’X’]$, and target variable using $data[’y’]$. Report $R^2$ for $k = 50, 100, 150$ on 5-fold cross validation. Use $C =identity$ for submodRegression. What happens if you change $C$ to a different diagonal matrix?\n",
    "\n",
    "1. fMRI data. Use $numpy.load$ like above to load the variables. Remember to use $data[’C’].item()$ to access the sparse csc matrix. Report $R^2 $for sparsity $k = {50, 100, 200}$.\n",
    "\n",
    "2. KDD cup 2008 dataset from https://archive.ics.uci.edu/ml/datasets/KDD+Cup+1998+Data.\n",
    "Use $cup98lrn.zip$ for both training/test. Randomly subsample 10, 000 rows to use as the training data, and another 5,000 rows to use as the test data. Replace all missing values with 0. Report $R^2$ for sparsity $k = {50, 100, 200}$. Use $C=identity$ for submodRegression.\n",
    "\n",
    "3. Blog feedback data.See https://archive.ics.uci.edu/ml/datasets/BlogFeedback for details.\n",
    "Randomly subsample 10,000 rows to use as the training data, and another 5,000 rows to use as the test data. Report $R^2$ for sparsity $k = {50, 100, 200}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No.1 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso:\n",
      "k=50, R-square=0.621039\n",
      "k=100, R-square=0.965618\n",
      "k=100, R-square=0.967176\n",
      "k=100, R-square=0.968654\n",
      "k=150, R-square=0.995615\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# from __future__ import print_function\n",
    "# print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data=np.load(\"Q3/simulated.npz\")\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state=20)\n",
    "\n",
    "k_fold = KFold(5)\n",
    "alphas =  10**np.linspace(10,-2,1000)*0.5\n",
    "lasso = Lasso()\n",
    "print (\"Lasso:\")\n",
    "for alpha in alphas:\n",
    "    lasso.set_params(alpha=alpha, random_state=20)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    k = 0\n",
    "    for coef in lasso.coef_:\n",
    "        if coef != 0:\n",
    "            k = k + 1\n",
    "    if k == 50 or k == 100 or k== 150:\n",
    "        r_square = 0\n",
    "        for i, (train, test) in enumerate(k_fold.split(X_train, y_train)):\n",
    "            lasso.fit(X_train[train], y_train[train])\n",
    "            r_square = r_square + lasso.score(X_train[test], y_train[test])\n",
    "        print (\"k=%d, R-square=%f\"% (k, r_square/5))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) ARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARDRegression\n",
      "k=50, R-square=0.784213\n",
      "k=100, R-square=0.983576\n",
      "k=150, R-square=0.989937\n",
      "k=150, R-square=0.990735\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ARDRegression\n",
    "print (\"ARDRegression\")\n",
    "\n",
    "def ard_FindK(X_train, y_train, k, thresholds):\n",
    "    ard = ARDRegression(compute_score=True)\n",
    "    for threshold in thresholds:\n",
    "        ard.set_params(threshold_lambda=threshold)\n",
    "        ard.fit(X_train, y_train)\n",
    "        k_count = 0\n",
    "        for coef in ard.coef_:\n",
    "            if coef != 0:\n",
    "                k_count = k_count + 1\n",
    "        if abs(k_count - k)<=5:\n",
    "            r_square = 0\n",
    "            for i, (train, test) in enumerate(k_fold.split(X_train, y_train)):\n",
    "                ard.fit(X_train[train], y_train[train])\n",
    "                r_square = r_square + ard.score(X_train[test], y_train[test])\n",
    "            print (\"k=%d, R-square=%f\"% (k, r_square/5))           \n",
    "\n",
    "ard_FindK(X_train, y_train, 50, 10 ** np.linspace(0.0, 0.5, 10))\n",
    "ard_FindK(X_train, y_train, 100, 10 ** np.linspace(2.0, 2.5, 10))\n",
    "ard_FindK(X_train, y_train, 150, 10 ** np.linspace(2.5, 3.0, 10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) SubMod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C is Identity:\n",
      "k=50, R-square=0.923545\n",
      "k=100, R-square=0.998770\n",
      "k=150, R-square=0.999065\n",
      "C is a different Diagonal Matrix:\n",
      "k=50, R-square=0.923547\n",
      "k=100, R-square=0.998774\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"Q3/codetoshare\")\n",
    "import Solver\n",
    "import submodRegression\n",
    "from Solver import solvePosteriorPrecisionSparseGreedy\n",
    "\n",
    "print (\"C is Identity:\")\n",
    "def submod_Identity(k):\n",
    "    square = 0\n",
    "    lr = linear_model.LinearRegression()\n",
    "    for i, (train, test) in enumerate(k_fold.split(X_train, y_train)):\n",
    "        coef = submodRegression.run_submodregression(X_train, y_train, k)\n",
    "        lr.fit(X_train[train], y_train[train])\n",
    "        lr.coef_ = coef\n",
    "        square = square + lr.score(X_train[test], y_train[test])\n",
    "    print (\"k=%d, R-square=%f\"% (k, square/5))  \n",
    "\n",
    "submod_Identity(50)\n",
    "submod_Identity(100)\n",
    "submod_Identity(150)\n",
    "\n",
    "def run_submodregression(Xtrain, ytrain, k, sigma=1):\n",
    "    r = np.dot(np.transpose(Xtrain), ytrain)/sigma\n",
    "    C= np.eye(np.shape(Xtrain)[1])*2\n",
    "\n",
    "    diag_xTx = np.array([0.0] * np.shape(Xtrain)[1])\n",
    "    for ii in range(len(diag_xTx)):\n",
    "        diag_xTx[ii] = np.dot(Xtrain[:, ii], Xtrain[:, ii])\n",
    "#     print \"done\"\n",
    "\n",
    "    where1, pmu= solvePosteriorPrecisionSparseGreedy(X = Xtrain, C=C, k = k, r=r, noiseVar=sigma, diag_xTx = diag_xTx, debug=0,opt_dict=None)\n",
    "\n",
    "    beta = np.zeros(np.shape(Xtrain)[1])\n",
    "    beta[where1] = pmu\n",
    "    return beta\n",
    "\n",
    "def submod_K(k):\n",
    "    square = 0\n",
    "    lr = linear_model.LinearRegression()\n",
    "    for i, (train, test) in enumerate(k_fold.split(X_train, y_train)):\n",
    "        coef = run_submodregression(X_train, y_train, k)\n",
    "        lr.fit(X_train[train], y_train[train])\n",
    "        lr.coef_ = coef\n",
    "        square = square + lr.score(X_train[test], y_train[test])\n",
    "    print (\"k=%d, R-square=%f\"% (k, square/5))  \n",
    "\n",
    "print (\"C is a different Diagonal Matrix:\")\n",
    "submod_K(50)\n",
    "submod_K(100)\n",
    "submod_K(150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Finding Decision Boundary (7 points)\n",
    "(1) (3 pts) Suppose samples in $R^2$ (the two-dimensional Cartesian space) are being obtained from two classes,\n",
    "C1 and C2, both of which are normally distributed with means at (1.5, 1) and (1, 1.5) respectively.\n",
    "The covariance matrix for each class is the same:\n",
    "$$\\Sigma_1 = \\Sigma_2 = \\begin{bmatrix} 4 & 0 \\\\ 0 & 4\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "If the priors of C1 and C2 are 4/7 and 3/7 respectively, what is the ideal (i.e. Bayes Optimal)\n",
    "decision boundary? (derive the equation for this boundary)\n",
    "\n",
    "(2) (4 pts) Suppose the cost of misclassifying an input actually belonging to C2 is twice as expensive as\n",
    "misclassifying an input belonging to C1. Correct classification does not incur any cost. If the\n",
    "objective is to minimize the expected cost rather than expected misclassification rate, what would\n",
    "be the best decision boundary? (obtain the equation describing this boundary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(1)\n",
    "\n",
    "At the decision boundary:\n",
    "$$ P(C_1|x) = P(C_2|x) $$\n",
    "and\n",
    "$$ P(C_1|x) = P(x|C_1)P(C_1)/p(x) $$\n",
    "$$ P(C_2|x) = P(x|C_2)P(C_2)/p(x)  $$\n",
    "so we get:\n",
    "$$ P(x|C_1)P(C_1) = P(x|C_2)P(C_2)$$\n",
    "since\n",
    "$$ P(C_1) = 4/7, P(C_2) = 3/7 $$\n",
    "$$ P(x|C_1) =  \\frac{1}{2\\pi |\\sum_1|^{1/2} } e ^{-\\frac{1}{2}(x - \\mu_1)^{T} \\sum_1^{-1}(x - \\mu_1)} $$\n",
    "$$ P(x|C_2) =  \\frac{1}{2\\pi |\\sum_2|^{1/2} } e ^{-\\frac{1}{2}(x - \\mu_2)^{T} \\sum_2^{-1}(x - \\mu_2)} $$\n",
    "then\n",
    "$$ \\frac{4}{7} \\frac{1}{2\\pi |\\sum_1|^{1/2} } e ^{-\\frac{1}{2}(x - \\mu_1)^{T} \\sum_1^{-1}(x - \\mu_1)} = \\frac{3}{7} \\frac{1}{2\\pi |\\sum_2|^{1/2} } e ^{-\\frac{1}{2}(x - \\mu_2)^{T} \\sum_2^{-1}(x - \\mu_2)}$$\n",
    "so the equation is:\n",
    "$$ 4e^{\\frac{(x_1 - 1.5)^2 + (x_2 - 1)^2}{8}} =  3e^{\\frac{(x_1 - 1)^2 + (x_2 - 1.5)^2}{8}} $$\n",
    "\n",
    "(2)\n",
    "\n",
    "The cost of misclassifying C2 is more expensive, in order to minimize the expected cost:\n",
    "$$ P(C_1|x) = 2P(C_2|x) $$\n",
    "implement the same process from (1), we get:\n",
    "$$ 2e^{\\frac{(x_1 - 1.5)^2 + (x_2 - 1)^2}{8}} =  3e^{\\frac{(x_1 - 1)^2 + (x_2 - 1.5)^2}{8}} $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
